# ğŸ API Flask â€“ Stack Overflow Tag Predictor  
*Par AgnÃ¨s Regaud â€“ Projet 5 â€“ OpenClassrooms â€“ Parcours IngÃ©nieur Machine Learning*

![Tests](https://github.com/agnesR23/OC_IML_P5_Stackoverflow_tags_prediction_api_flask/actions/workflows/test.yml/badge.svg?branch=main)



Ce rÃ©pertoire contient le code source de lâ€™API Flask qui expose un service REST permettant de prÃ©dire automatiquement les tags dâ€™une question Stack Overflow Ã  partir de son titre et de son contenu.

## ğŸ¯ Objectif

Fournir une API REST simple et robuste pour recevoir une question (titre + corps) et retourner des prÃ©dictions de tags via deux modÃ¨les : supervisÃ© (CatBoost) et non supervisÃ© (NMF).

## ğŸ“ Contenu du rÃ©pertoire

- `app.py` : point dâ€™entrÃ©e de lâ€™API Flask
- `artifacts/` : modÃ¨les entraÃ®nÃ©s et objets de prÃ©traitement sauvegardÃ©s, non versionnÃ©
- `environment.yml` : environnement conda complet pour lâ€™API
- `environment-tests.yml` : environnement lÃ©ger dÃ©diÃ© aux tests unitaires
- `tests/` : tests unitaires Pytest
- `Dockerfile` : dÃ©finition de lâ€™image Docker pour lâ€™API
- `README.md` : ce fichier

## â–¶ï¸ Lancement local (conda)

```bash
conda env create -f environment.yml
conda activate flask_app_env
python app.py

#Lâ€™API sera accessible sur : http://localhost:5001

**Important :**
Avant de lancer lâ€™API en local, vous devez donc rÃ©cupÃ©rer ou gÃ©nÃ©rer les artefacts et les placer dans le dossier artifacts/

ğŸ§ª Tests unitaires
conda env create -f environment-tests.yml
conda activate stackoverflow_tests
pytest

ğŸ³ Docker
docker build -t app_flask .
docker run -p 5001:5001 app_flask


ğŸ” Tests automatisÃ©s
Ce projet comprend une suite de tests unitaires automatisÃ©s avec pytest, couvrant les aspects suivants :

âœ… PrÃ©traitement des donnÃ©es
VÃ©rification que la fonction normalize_text nettoie et transforme les textes comme attendu.

Cas testÃ©s : suppression des majuscules, des caractÃ¨res spÃ©ciaux, des balises HTML, etc.

âœ… API Flask de prÃ©diction de tags
- /predict avec CatBoost : vÃ©rification des rÃ©ponses et de la structure (tags prÃ©dits, scores, seuils, etc.).

- /predict avec NMF : vÃ©rification de la bonne gestion du modÃ¨le non supervisÃ© et rÃ©ponse correcte sans seuil (threshold=None).

- Gestion des erreurs : champs manquants, artefacts non chargÃ©s, etc.

- Isolation des composants externes via mocks dans les tests


âš™ï¸ IntÃ©gration continue
Les tests sont exÃ©cutÃ©s automatiquement via GitHub Actions Ã  chaque push grÃ¢ce Ã  un workflow CI (python-app.yml).